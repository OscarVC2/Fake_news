{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaIBmnXCknPl"
   },
   "source": [
    "About the Dataset:\n",
    "\n",
    "1. id: unique id for a news article\n",
    "2. title: the title of a news article\n",
    "3. author: author of the news article\n",
    "4. text: the text of the article; could be incomplete\n",
    "5. label: a label that marks whether the news article is real or fake:\n",
    "           1: Fake news\n",
    "           0: real News\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k399dHafvL5N"
   },
   "source": [
    "Importing the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-fetC5yqkPVe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1AC1YpmGwIDw",
    "outputId": "d726f6f2-2aae-4146-ae10-4776179d07d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oscarandresvalladarescardona/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxIOt3DowpUR",
    "outputId": "b3cf98ca-97a7-4635-aaa1-d6454a8759e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# printing the stopwords in English\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjeGd1CLw_6R"
   },
   "source": [
    "Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nCGcpu_1wzLw"
   },
   "outputs": [],
   "source": [
    "# loading the dataset to a pandas DataFrame\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRgmbYSbxV4-",
    "outputId": "82bafe4f-211d-47b8-f4ed-a61b77370bc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "jjJ1eB6RxZaS",
    "outputId": "9bb0c756-30e5-4919-a8e0-a688127261eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYkDi4SwxlKi",
    "outputId": "204c4b11-2d09-4c3e-ff1f-4ee9d3c5bc92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting the number of missing values in the dataset\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Mc04lQrhx57m"
   },
   "outputs": [],
   "source": [
    "# replacing the null values with empty string\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "H7TZgHszygxj"
   },
   "outputs": [],
   "source": [
    "# merging the author name and news title\n",
    "df['content'] = df['author']+' '+df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ga_DaZxhzoWM"
   },
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zY-n0dCh0e-y"
   },
   "outputs": [],
   "source": [
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MBUIk4c94yTL"
   },
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xmwK-zyO5Stg",
    "outputId": "61de12b9-b9fa-486f-8c91-a5aec48d7fe6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        darrel lucu hous dem aid even see comey letter...\n",
      "1        daniel j flynn flynn hillari clinton big woman...\n",
      "2                   consortiumnew com truth might get fire\n",
      "3        jessica purkiss civilian kill singl us airstri...\n",
      "4        howard portnoy iranian woman jail fiction unpu...\n",
      "                               ...                        \n",
      "20795    jerom hudson rapper trump poster child white s...\n",
      "20796    benjamin hoffman n f l playoff schedul matchup...\n",
      "20797    michael j de la merc rachel abram maci said re...\n",
      "20798    alex ansari nato russia hold parallel exercis ...\n",
      "20799                            david swanson keep f aliv\n",
      "Name: content, Length: 20800, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5ZIidnta5k5h"
   },
   "outputs": [],
   "source": [
    "#separating the data and label\n",
    "X = df['content'].values\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BMfepsQZ6TES"
   },
   "outputs": [],
   "source": [
    "# converting the textual data to numerical data\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X)\n",
    "X = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VjMYwmBo7Pbx"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'rf': make_pipeline(RandomForestClassifier(random_state=42)),\n",
    "    'gb': make_pipeline(GradientBoostingClassifier(random_state=42)),\n",
    "    'l1': make_pipeline(StandardScaler(), LogisticRegression(penalty='l1', random_state= 1,solver='liblinear')),\n",
    "    'l2': make_pipeline(StandardScaler(), LogisticRegression(penalty='l2', random_state= 1,solver='liblinear')),\n",
    "    'elasticnet': make_pipeline(StandardScaler(), LogisticRegression(penalty='l2', random_state= 1,solver='liblinear')), \n",
    "    'dt': make_pipeline(DecisionTreeClassifier(random_state=42)),\n",
    "    'knn': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "    'svc':make_pipeline(StandardScaler(), SVC(random_state=42))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf <class 'sklearn.pipeline.Pipeline'>\n",
      "gb <class 'sklearn.pipeline.Pipeline'>\n",
      "l1 <class 'sklearn.pipeline.Pipeline'>\n",
      "l2 <class 'sklearn.pipeline.Pipeline'>\n",
      "elasticnet <class 'sklearn.pipeline.Pipeline'>\n",
      "dt <class 'sklearn.pipeline.Pipeline'>\n",
      "knn <class 'sklearn.pipeline.Pipeline'>\n",
      "svc <class 'sklearn.pipeline.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "for key, value in pipelines.items():\n",
    "    print(key, type(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_hyperparameters = {\n",
    "    'randomforestclassifier__criterion':['entropy','gini'],\n",
    "    'randomforestclassifier__n_estimators': [100, 300],\n",
    "    'randomforestclassifier__max_features': ['auto', 0.3, 0.6],\n",
    "    'randomforestclassifier__min_samples_split':[2,3,10],\n",
    "    'randomforestclassifier__min_samples_leaf':[1,3,10],\n",
    "    'randomforestclassifier__bootstrap':[False]\n",
    "}\n",
    "\n",
    "gb_hyperparameters = {\n",
    "    'gradientboostingclassifier__n_estimators':[100, 500, 100],\n",
    "    'gradientboostingclassifier__learning_rate': [0.001,0.01,0.1,0.05],\n",
    "    'gradientboostingclassifier__max_depth':[3, 5, 10]\n",
    "}\n",
    "\n",
    "l1_hyperparameters = {\n",
    "    'logisticregression__C': np.logspace(-3,3,7)\n",
    "}\n",
    "l2_hyperparameters = {\n",
    "    'logisticregression__C': np.logspace(-3,3,7)\n",
    "}\n",
    "\n",
    "elasticnet_hyperparameters = {\n",
    "    'logisticregression__C': np.logspace(-3,3,7)\n",
    "}\n",
    "\n",
    "dt_hyperparameters = {\n",
    "    \"decisiontreeclassifier__max_depth\" : [1,3,5,7],\n",
    "    'decisiontreeclassifier__criterion':['entropy','gini'],\n",
    "    'decisiontreeclassifier__splitter':['best']\n",
    "}\n",
    "\n",
    "knn_hyperparameters = {\n",
    "    \"kneighborsclassifier__n_neighbors\": np.arange(1,50),\n",
    "    \"kneighborsclassifier__weights\": [\"uniform\",\"distance\"]\n",
    "}\n",
    "\n",
    "svc_hyperparameters = {\n",
    "    'svc__C': [1.0,], \n",
    "    'svc__kernel': ['rbf',], \n",
    "    'svc__degree': [2,], \n",
    "    'svc__gamma' : [1.0,], \n",
    "    'svc__shrinking':[True,], \n",
    "    'svc__probability':[True,],\n",
    "    'svc__tol': [0.001,],\n",
    "    'svc__cache_size':[200,]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_grids = {\n",
    "    'rf': rf_hyperparameters,\n",
    "    'gb': gb_hyperparameters,\n",
    "    'l1':l1_hyperparameters,\n",
    "    'l2':l2_hyperparameters,\n",
    "    'elasticnet':elasticnet_hyperparameters,\n",
    "    'dt':dt_hyperparameters,\n",
    "    'knn': knn_hyperparameters,\n",
    "    'svc':svc_hyperparameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf was found, and it is a grid.\n",
      "gb was found, and it is a grid.\n",
      "l1 was found, and it is a grid.\n",
      "l2 was found, and it is a grid.\n",
      "elasticnet was found, and it is a grid.\n",
      "dt was found, and it is a grid.\n",
      "knn was found, and it is a grid.\n",
      "svc was found, and it is a grid.\n"
     ]
    }
   ],
   "source": [
    "for key in ['rf', 'gb', 'l1', 'l2', 'elasticnet', 'dt', 'knn', 'svc']:\n",
    "    if key in hyperparameter_grids:\n",
    "        if type(hyperparameter_grids[key]) is dict:\n",
    "            print( key, 'was found, and it is a grid.' )\n",
    "        else:\n",
    "            print( key, 'was found, but it is not a grid.' )\n",
    "    else:\n",
    "        print( key, 'was not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['rf', 'gb', 'l1', 'l2', 'elasticnet', 'dt', 'knn', 'svc'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {}\n",
    "\n",
    "for key in pipelines.keys():\n",
    "    models[key]= GridSearchCV(pipelines[key], hyperparameter_grids[key], cv=10, scoring = 'roc_auc',\n",
    "                             n_jobs= -1,\n",
    "                             verbose = 1)\n",
    "    \n",
    "models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 108 candidates, totalling 1080 fits\n"
     ]
    }
   ],
   "source": [
    "for key in models.keys():\n",
    "    clf = models[key].fit(X_train,y_train)\n",
    "    print(key, \"is trained and tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score,\\\n",
    "                            roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in models.keys() :\n",
    "    pred = models[key].predict(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "    print(key)\n",
    "    print(\"Score del modelo (accuracy):\", round(models[key].score(X_test, y_test), 3))\n",
    "    print(\"Accuracy score:\", round(accuracy_score(pred, y_test), 3))\n",
    "    print(\"Recall score:\", round(recall_score(pred, y_test), 3))\n",
    "    print(\"Precision score:\", round(precision_score(pred, y_test), 3))\n",
    "    print(\"F1 score:\", round(f1_score(pred, y_test), 3))\n",
    "    print('AUROC =', round(auc(fpr, tpr), 4))\n",
    "    print(\"AUC:\", round(roc_auc_score(pred, y_test), 3)) \n",
    "    c_matrix = confusion_matrix(y_test,pred)\n",
    "    sns.heatmap(c_matrix/np.sum(c_matrix), annot=True, fmt='.2%', cmap='Blues')\n",
    "    print('---')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project 5. Fake News Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
